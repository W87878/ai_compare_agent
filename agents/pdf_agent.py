from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
import asyncio
from langchain.prompts import SystemMessagePromptTemplate
from langchain.schema import HumanMessage, SystemMessage
from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler
from dotenv import load_dotenv
load_dotenv()  # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
import os
# ç¢ºä¿å·²ç¶“è¨­å®šäº† OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # å–å¾—ç’°å¢ƒè®Šæ•¸çš„å€¼
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY environment variable is not set.")
from rag.vectorstore import build_vectorstore  # ä½ è‡ªå·±çš„ FAISS è¼‰å…¥å™¨

async def run_retriever(question, file_name1, file_name2):
    # Step 1: æº–å‚™å‘é‡è³‡æ–™åº«èˆ‡æª¢ç´¢å™¨
    vectordb1 = build_vectorstore(file_name1)
    retriever1 = vectordb1.as_retriever()
    vectordb2 = build_vectorstore(file_name2)
    retriever2 = vectordb2.as_retriever()

    def format_docs(docs: list[Document]) -> str:
        return "\n\n".join(doc.page_content for doc in docs)

    # ç”¨é streaming LLM åŸ·è¡Œå…©æ®µå›ç­”ï¼ˆå…ˆåŸ·è¡Œå¥½ RAGï¼‰
    static_llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0)

    # Step 3: å°å…©ä»½æ–‡ä»¶å€‹åˆ¥åŸ·è¡Œ RAG æŸ¥è©¢
    system_prompt = SystemMessagePromptTemplate.from_template(
    template= """
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ–‡ä»¶è™•ç†å“¡ï¼Œè«‹é‡å°é€™ä»½æ–‡ä»¶ï¼Œä¾æ“šä½¿ç”¨è€…çš„å•é¡Œï¼Œåˆç†å›ç­”å•é¡Œï¼Œå‹™å¿…ç¨±å‘¼ç‚ºã€é€™ä»½æ–‡ä»¶ã€ï¼š

        PDF å…§å®¹ï¼š
        {context}

        å•é¡Œï¼š
        {question}

        è«‹å›ç­”ï¼š
        """,
        template_format="f-string"  # ğŸ‘ˆ æ˜ç¢ºæŒ‡å®šæ ¼å¼
    )

    base_prompt = ChatPromptTemplate.from_messages([
        system_prompt
    ])
    
    base_chain = (
        {"context": retriever1 | RunnableLambda(format_docs), "question": RunnablePassthrough()}
        | base_prompt
        | static_llm
        | StrOutputParser()
    )

    base_chain2 = (
        {"context": retriever2 | RunnableLambda(format_docs), "question": RunnablePassthrough()}
        | base_prompt
        | static_llm
        | StrOutputParser()
    )
    
    answer1 = await base_chain.ainvoke(question)
    answer2 = await base_chain2.ainvoke(question)
    
    return {'answer1':answer1, 'answer2':answer2}

from openai import AsyncOpenAI
openai = AsyncOpenAI(api_key=OPENAI_API_KEY)

async def pdf_qa_agent_stream(question, answer1, answer2, memory):
    prompt = f"""
    æ ¹æ“šä»¥ä¸‹æ–‡ä»¶å…§å®¹å›ç­”ä½¿ç”¨è€…å•é¡Œï¼š
    æ–‡ä»¶1: {answer1}
    æ–‡ä»¶2: {answer2}
    
    å•é¡Œ: {question}
    """
    response = await openai.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        stream=True,
    )
    full_response = ''
    i = 0
    async for chunk in response:
        delta = chunk.choices[0].delta.content or ""
        if delta != '':
            full_response += delta
            i += 1
            yield delta

    memory.save_context({"input": question}, {"output": full_response})

# async def pdf_qa_agent_stream(question, answer1, answer2, memory):
#     # Step 1: Streaming LLM
#     callback = AsyncIteratorCallbackHandler()
#     streaming_llm = ChatOpenAI(
#         api_key=OPENAI_API_KEY,
#         temperature=0,
#         streaming=True,
#         callbacks=[callback]
#     )

#     # Step 2: æ¯”è¼ƒå…©ä»½å›æ‡‰
#     comparison_prompt = f"""
#         ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ¯”è¼ƒåˆ†æåŠ©ç†ï¼Œè«‹æ¯”è¼ƒå…©ä»½ pdf é‡å°ä»¥ä¸‹å•é¡Œçš„å›æ‡‰å·®ç•°ï¼š

#         ä½¿ç”¨è€…å•é¡Œï¼š{question}

#         ç¬¬ä¸€ä»½æ–‡ä»¶å›æ‡‰ï¼š
#         {answer1}

#         ç¬¬äºŒä»½æ–‡ä»¶å›æ‡‰ï¼š
#         {answer2}

#         è«‹ç¸½çµå·®ç•°ï¼Œä¸¦æŒ‡å‡ºå“ªä¸€ä»½è³‡æ–™è¼ƒå®Œæ•´æˆ–æ¸…æ¥šã€‚
#     """

#     # prompt = ChatPromptTemplate.from_messages([
#     #     SystemMessagePromptTemplate.from_template(comparison_prompt)
#     # ])

#     # rag_chain = (
#     #     prompt
#     #     | streamimg_llm
#     #     | StrOutputParser()
#     # )

#     # Step 3: å•Ÿå‹•ç”Ÿæˆ + å›å‚³ä¸²æµ token
#     full_response = ""
#     # for debug
#     # Step 5: å•Ÿå‹•ç”Ÿæˆ + å›å‚³ä¸²æµ token
#     # ç”¢ç”Ÿ ChatMessageï¼Œåªæœ‰ system messageï¼ˆæˆ–å¯ä»¥åŒ…æˆ user messageï¼‰
#     messages = [SystemMessage(content=comparison_prompt)]

#     # å•Ÿå‹•éåŒæ­¥ç”Ÿæˆä¸¦ç­‰å¾…å®Œæˆ
#     invoke_task = asyncio.create_task(streaming_llm.ainvoke(messages))

#     full_response = ""
#     try:
#         async for chunk in callback.aiter():
#             full_response += chunk
#             yield chunk
#     finally:
#         # ç¢ºä¿ invoke_task å®Œæˆï¼Œé¿å…è­¦å‘Šæˆ–æœªçµæŸä»»å‹™
#         await invoke_task
#     memory.save_context({"input": question}, {"output": full_response})

if __name__ == '__main__':
    print(0)